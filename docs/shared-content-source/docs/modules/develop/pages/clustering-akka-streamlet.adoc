:page-partial:
:page-supergroup-scala-java: Language

include::ROOT:partial$include.adoc[]

Any application that processes a stream of events and does not just perform trivial record-at-a-time
transformations needs to be stateful, with the ability to store and access intermediate data.

Although Akka Streams by itself is stateless, there are many ways to implement stateful streaming
in Akka Streams by leveraging existing constructs like
https://doc.akka.io/docs/akka/current/stream/operators/Source-or-Flow/statefulMapConcat.html[statefulMapConcat] or a
https://doc.akka.io/docs/akka/current/stream/stream-customize.html[custom graph stage].
However, the most straightforward and natural way of doing this is integration with
https://doc.akka.io/docs/akka/current/typed/index.html[Akka actors].
In this case, Akka Streams manage the overall execution flow, while individual actors both maintain
state and implement stateful operations. Coupled with
https://doc.akka.io/docs/akka/current/typed/index-persistence.html[Akka Persistence], this approach
provides a great foundation of stateful, fault-tolerant stream processing using Akka streams.
In order to allow scaling of the Akka streams implementations, leveraging actors we also
need to implement https://doc.akka.io/docs/akka/current/typed/cluster.html[Akka Cluster]
- a feature of Akka that allows Akka nodes to form a cluster.

== Akka Cluster

WARNING: The features on this page feature are experimental and may change.  It is not recommended to use these features in production just yet.

=== Use Case

Akka Clustering in a Cloudflow Akka Streamlet gives you the ability to leverage existing Akka tools for managing
data in-memory. Akka Cluster Sharding can be used where data must be consistent and Akka Distributed Data
can be used for eventually consistent highly-available data.  There exist many real world use cases where
stateful streams are needed, including model updates in real-time ML serving and sharding IOT Device State.

Add the Clustering trait to your streamlet as shown below to activate Clustering on a specific Akka Streamlet.

[source,scala]
----
object ConnectedCarCluster extends AkkaStreamlet with Clustering
----

By including the Clustering trait on your Akka Streamlet, Cloudflow will automatically setup all the
configuration your streamlet needs to form a cluster, both when running the application locally and when the application is deployed to a Kubernetes cluster.

== Kafka External Sharding Source

When using Akka Cluster Sharding with a normal Cloudflow source it is likely that the location of a specific
Akka shard is not the same as the node assigned to that shard's corresponding Kafka partition.  In this case every
message read from kafka would need to be sent over the network to the Shard node and back.

To avoid this case and coordinate the location of Akka shards and Kafka partitions you can
use `shardedSourceWithCommittable` and `shardedPlainSource`.  These Sources use an Akka feature
called a
https://doc.akka.io/docs/alpakka-kafka/current/cluster-sharding.html[Kafka ExternalShardAllocationStrategy].

To use these Sources you must first implement Clustering on your Akka Streamlet and then follow
the examples below.

Like its non-sharded counterpart, `shardedSourceWithCommittableContext` will include Kafka offset information
as context as well as implementing the Kafka Sharding strategy.

[source,scala]
----
val entity = Entity(typeKey)(createBehavior = entityContext => ConnectedCarActor(entityContext.entityId))

val source:SourceWithContext[ConnectedCarERecord, CommittableOffset, _] = shardedSourceWithCommittableContext(in, entity)
----

`shardedPlainSource` does not include any Kafka offset information as context but implements the Kafka
Sharding Strategy in the same way.

[source,scala]
----
val entity = Entity(typeKey)(createBehavior = entityContext => ConnectedCarActor(entityContext.entityId))

val source:Source[ConnectedCarERecord, _] = shardedPlainSource(in, entity)
----
== Akka Persistence

https://doc.akka.io/docs/akka/2.6.11/typed/index-persistence.html[Akka persistence]
allows to persist the state of actors, which enables both restore actors in the case of
failures and move actors from one instance to another (used for scaling).
Akka persistence supports multiple options for the database backend, including
https://doc.akka.io/docs/akka-persistence-cassandra/current/index.html[Cassandra]
and several
https://doc.akka.io/docs/akka-persistence-cassandra/current/index.html[relational databases]
- Postgres, MySQL, H2, Oracle, SQL Server.

A typical implementation of Akka persistence requires:

* Installation of the desired database server.
* Creation and initialization (including creation of the required tables) of the database.
* Creating of Akka configuration for using this database.
* Adding required jar file for Akka persistence support

By default Cloudflow automates some of these steps, including adding required jars, creation of
Akka configuration, creation of the database tables. In the most simplistic case, a user
can just provide a database server with a dedicated database and a configurate.
Configuration allows to specify the following parameters:

* Database type - `dbtype`. Required parameter, that specified a type of database used. Allowed
parameters are: `cassandra`, `postgres`, `mysql`, `h2`, `oracle` and `sqlserver`. All other values
will be ignored
* Database host - `host`. Optional parameter specifying database host location - defaulted to `localhost`,
ignored for H2
* Database port - `port`. Optional parameter specifying port, that database engine is listening on. Defaulted to
default port for a given database type, for example 3306 for Oracle, ignored for H2
* Database name - `database` - optional parameter, specifying database name (keyspace name for Cassandra).
defaulted to `akkapersistence`, ignored for H2
* Database user name - `user`  Optional parameter specifying user for database access. Defaulted to
default user for a given database type, for example root for H2
* Database user password - `password`. Optional parameter specifying password for database access. Defaulted to
default user for a given database type, for example root for H2

As a result H2 database can be configured (for akka runtime) as:


----
cloudflow.streamlets.processor.config.akka{
  persistence{
    dbtype = h2
  }
}
----

And for local Cassandra, with default user name/password (for akka runtime) as:

----
cloudflow.runtimes.akka.config.akka{
  persistence{
    dbtype = cassandra
  }
}
----

If you  do not like defaults, you can, of course, create a custom Akka configuration
directly in your project, that will be used in place of default implementation.

[NOTE]
====
In order to accomodate automatic tables creation, a new parameter `auto-create-tables` is
added to the `akka-persistence-jdbc.shared-databases.slick` Default value for this parameter is
`false`. If you are writing your custom persistence configuration, use this parameter to control
table creation, following snipet below:
----
akka-persistence-jdbc {
    shared-databases.slick {
        auto-create-tables = true
        profile = "slick.jdbc.PostgresProfile$"
        db {
            driver = "org.postgresql.Driver"
            url = "jdbc:postgresql://"${example.proxy.postgres.service}":"${example.proxy.postgres.port}"/"${example.proxy.postgres.database}
            user = ${example.proxy.postgres.user}
            password = ${example.proxy.postgres.password}
            ssl = ${example.proxy.postgres.useSSL}
            sslmode = ${example.proxy.postgres.sslMode}
            sslrootcert = ${example.proxy.postgres.cert}
            numThreads = 5
            maxConnections = 5
            minConnections = 1
            connectionTimeout = 5000
        }
    }
}
----
====